* Overview
  This is my attempt at "the matasano crypto challenges" at
  cryptopals.com

* Set 1
** Convert hex to base64
*** 2015-11-01
    Base64 uses 6 bits, vs normal hex (Base16) which uses 4. Need to
    convert bytes (8 bits, Base256) to 6 bit words. Best conversion is
    24 bits which is 3x8 and 4x6.

*** 2015-11-02
    Break byte array into hexlets.
    Next: need to refactor convert-hexlet!
    Saw an interesting way to manage the Base64code. Make an array of
    all characters

*** 2015-11-03
    Use partition function to split string into 3 byte groups. Then pad
    the tuples(?) with "0"s. I tried the following code, but removed
    it in preference for another version:
    #+BEGIN_SRC clojure
   (defn hex-3split
  [[b1 b2 b3]]
  (let [x (bit-or
           (bit-shift-left (int b1) 16)
           (bit-shift-left (int b2) 8)
           (bit-shift-left (int b3) 0))]
    [(unsigned-bit-shift-right x 18)
     (bit-and (unsigned-bit-shift-right x 12) 0x3f)
     (bit-and (unsigned-bit-shift-right x 6) 0x3f)
     (bit-and x 0x3f)]))
    #+END_SRC

*** 2015-11-04
    Think I've got base64-encode working. I'm now starting on base16
    decode.
    I've used a Java string conversion routine for hex conversion. I
    got set 1 test to pass!

** Fixed XOR
***   2015-11-04
      Started and finished. Just needed to add a formatting encode-base16 as
      well.

** Single-byte XOR cipher
***   2015-11-04
    Interesting! Need to break a cipher. Suggested to use character
    frequency, so need to look that up...
    Letter frequency table came from:
    http://www.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html
    My idea to score the result text is:
1. calculate score for each letter in text
2. calcualte relative score for each letter
3. calculate absolute difference in relative score for each letter (between
   known frequency and text)
4. sum difference
5. smallest difference wins

*** 2015-11-05
    Looking for a way to convert between string and keyword in
    clojure. Found "keyword", but not sure it's what I want.
    Another way to solve this might be to iterate over list and filter on
    each item in the iteration. This means at most 26ish repetitions (not
    sure about digits).
    Created function to count occurences of characters in a string and
    return character and count in map.

*** 2015-11-06
    Got word-score working. It can be a generic function that compares
    two maps, comparing the second value of each map.
    I hacked a bit of a pay around to try and get the algorithm. CAN'T
    LEAVE IT LIKE THIS!
*** 2015-11-07
    I've seen the code decrypted, but my frequency algorithm is not
    quite working correctly. It may be that I'm not creating a
    distribution of the code letters, and am just comparing absoute
    counts. Also, not sure how to account for punctuation in frequency
    distribution.
    Cleaning up some of the earlier code. Especially the base64
    decode.
*** <2015-11-08>
    + Detour - play around with concept of tranducers. Started
      watching R.Hickey talk on this. Recommends "Lectures on
      Constructive Functional Programming" by R.S
      Bird. http://www.cs.ox.ac.uk/files/3390/PRG69.pdf and A tutorial
      on the universality and expressiveness of fold by GRAHAM HUTTON
      -- http://www.cs.nott.ac.uk/~gmh/fold.pdf
    + I've just commited change with commented old functions. I will
      now remove these, and just leave transducers.
    + I was really sick of the encode-byte routine. So I ran it, and
      copied the output directly in. Much cleaner. For reference the
      original was:
      #+BEGIN_SRC clojure
        (defn encode-byte-base64
          [b]
          (let [crs [[\A \Z] [\a \z] [\0 \9]]
                rfn (fn [[a b]] (range (byte a) (inc (byte b))))
                rconj (fn [x y m] (conj (vec  m) x y))]
            (nth (->>
                  crs
                  (map rfn)
                  (rconj \+ \/)
                  flatten
                  (map char)) b)))
      #+END_SRC
      + Found function "frequencies" which counts distinct occurences
        of items in sequence. Replaced Count-occurences. Previous code
        was:
      #+BEGIN_SRC clojure
        (defn count-occurences
          [m l]
          (let [h (first l)
                f (fn [a] (= h a))]
            (cond
              (some? h) (count-occurences
                         (assoc m h (count (filter f l)))
                         (remove f l))
              :else m)))

        (deftest test-count-occurences
          (testing "test count-occurence"
            (are [x y] (= x (count-occurences {} y))
              {\a 1} "a"
              {\a 5} "aaaaa"
              {} ""
              {\a 3 \b 3} "ababab")))
      #+END_SRC
*** <2015-11-09>
    + I need to create a function that will diff two maps. Should
      return either a map, or a sequence of differences.
      1. Use the input map as base
      2. for each key, output a new key that is value - value of same
         key in reference map
      3. If reference map does not have the key, then assume the
         referene map has 0 as the value.
    + I've found the merge-with function. this is perfect for my
      needs. It takes two maps, and merges them while applying a
      function.
*** <2015-11-10>
    + Finding it hard to create a test for the XOR word
      function. There's definately something wrong with my scoring
      function. I'm just not sure what. Why is everything scoring
      9.9172..?
*** <2015-11-11>
    + I think i've got character conversion issues. Not sure how my
      tests haven't picked this up. I'll have to review, and find
      why. Still not getting a good word score!
    + Finally got it working. Was a mix of dealing with special
      characters correctly and a good comparison function. Trick was
      to remove whitespace, but leave non-letters.This penalises
      non-letters but ignore whitespace. Also, comparison function is
      divide. This works better than - gives a better differentiator.
    + Code needs a clean-up. Maybe some more tests too?? Definately a
      clean up.
    + Anyway, challenge complete!
** Detect single-character XOR
*** <2015-11-12>
    + Probably the main steps for this challenge are:
      - Slurp file
      - For each line in file find the decode-byte and word score
      - Return the line, byte and score for every line
      - Take the line with the maximum score
    + Did a bit of clean-up and checked in.
*** <2015-11-19>
    + Can make this Lazy I think. Run the decoder in the file read
      loop.
    + I've hacked together a solution, but it is pretty slow. I'm sure
      i can come up with something much better.
    + The challenge test runs slow (1000ms?), so I've disabled the
      test. Again I think this is better servered with a refactor.
    + I'm still not convinced about my code breakdown. I want more
      useful, small functions I can tie together.
** Implement repeating-key XOR
*** <2015-11-19>
    + I have a small error where the Hex string representation is not
      including leading 0s on conversions
    + I changed the Hex encode function. Was simply change in string
      from "%h" to "%02x".
    + The text was correct, but a hidden \n threw me off.
    + Challenge complete
** Break repeating-key XOR
*** <2015-11-20>
    + First task is to get Hamming distance between two words. I think
      this is straight forward using Integer.bitCount from Java. Yes,
      it was pretty simple.
    + To solve the main problem I just need to compose existing items:
      - Slurp file
      - Decode from base64
      - Find the keysize
        - Get the number between 2 and 40 with the minimum hamming
      - Break the code into blocks
      - Solve each block as single - keep the decode byte
      - Combine the decode bytes
    + I need to create a decode-base64 function. I've only got a
      decode function at the moment.
    + Actually, I found the Java 8 function
      Base64.Decoder.decode. Have created a wrapper that converts
      input to string, and output to vector. Nice and simple.
    + I've put together a skeleton, but it's not right. I've run it,
      and get jibberish results. Not sure what's going wrong. Assume
      the key i'm discovering is wrong, as I'd expect challenge to use
      an actual word as a key. The current key I'm getting is :[95 83
      94 84 94]
*** <2015-11-27>
    + It's been a while. I've been working on a presentation for RSS,
      and in the mean time I have learnt a lot about R statistics
      language. I think I will give it a go plotting some of the
      output of my algoriths.
    + I plotted output. The outlier is not very clear. I'm wondering
      if my frequency difference algorithm needs tweaking. I'll have a
      look at this.
*** <2015-11-28>
    + Started twaking a copy of my frequency difference table. Not
      ready yet, so I commented it back out.
*** <2015-11-29>
    + Have been playing a lot with R on this. Noticed some interesting
      things. The SD and variance on letter frequencies is the same
      for every encoded string. This makes sense because we're
      modifing all letters exactly the same way with the XOR
      byte.
      This leads to the simple conclusion that we can't score phrases
      as english without taking into consideration the frequencies of
      actual letters (i.e. can't just score based on spread, mean, etc).
    + Have also found, which I'd ignored, the frequencies for spaces,
      white-space, and other punctuation. The "Space" is
      1.918182% (this treats carriage return as space). In fact, all
      non-letter characters (including space) represent 6.57%.
    + Next nice little exercise in R would be to load in the letter
      frequencies and actually graph a comparison.
*** <2015-11-30>
    + Have got a simple comparison of known letter frequency and
      decoded message letter frequency. The method I was using does
      not give a great answer. There is no "stand-out" winner, in
      terms of phrase score.
*** <2015-12-01>
    + Looking at score distributions, I think I really need a way to
      take non-valid characters into account. Some sort of score
      penalty for each non-printable character? Or maybe subtract
      non-printable characters from the total. Or maybe subtract all
      whitespace, except " ", from total?
*** <2015-12-02>
    + I've tried a Chi-Test on this, and also removed the comparison
      of relative (by multiplying by string length C=34). I'm still
      getting two out-liers. This definately needs a extra test on the
      types of characters included.
    + Another type of plot that will be interesting is character
      spread coloured by score. This will mean the columns are
      coloured, but the rows will show which characters are being
      used. I could also colour the values by character range..
*** <2015-12-03>
    + I've modified my Chi-Test. It now uses two values for NA in the
      expected value. It uses 0 in numerator, and 1 in
      denominator. Currently, this is two conditionals, but I'm sure
      can make this better. Anyway, result is now have a better
      outlier (minimum in this case) to identify.
*** <2015-12-05 Sat>
    + I did quite a bit last night, but didn't journal it. Not a good
      habbit to get into!
    + Last night I cleaned up the R code. Also, I cleared the
      buffer. This was actually a long task, as the buffer was huge
      (300k?). I had to force emacs to close, as it was unresponsive.
    + I've removed a lot of the junk from core.r. Down to 91 lines
      including plot.
    + The plot is now showing progression of the score over each
      iteration. This is interesting - the winning score does not
      shine until quite late in the scoring. I wonder if there is a
      better way? Or maybe a way to rule out sequences early in the
      process? Another idea is maybe create a bounding function that
      excludes falses early.
*** <2015-12-06 Sun>
    + I've installed Criterium. This is a clojure benchmarking
      tool. The idea being I can collect and benchmark the various
      algorithms for finding english
      phrases. https://github.com/hugoduncan/criterium/
    + Powershell command number lines in file:
      #+begin_src
      Get-Content test/clojure_crypto_challenge/4.txt | Measure-Object -Line
      #+end_src
    + Just found problem in core.clj. Lucky for source control. Was
      able to compare against previous commit. For future reference
      the command was "
      #+begin_src
      git diff HEAD^1 -- .\src\clojure_crypto_challenge\core.clj"
      #+end_src
    + I modified the R program, and think I have a nicer
      function. Instead of using the actual expected value of the
      letters - which for small strings always evaluates less than 1 -
      I use a rounded value. There are differences between ceiling,
      floor, and round. Round works the best - but must be carefull to
      treat "0" as "NA" (so the Chi score = x^2, not (x-y)^2/y -which
      would go infinity).
      + Also, the bounding function seems to be a straight line. Maybe
        at 2.5? Would need to tweak this, as it will be skewed by
        selected sample.
    + Have theory that I can make reducer/transducer that will iterate
      over sequences to produce score incrementally. I've proven in R
      that I can make the relative frequency scoring a bit
      inconsitent. That is: I apply the current length of the string
      to the expected frequency of a character only when that
      character changes. For example, if I have [a b c a] then the
      expected frequency of a will be 1*ra, b=2*rb, c=3*rc, then
      a=4*ra. The correct way would be to reapply the length of the
      string each time, but it is much more expensive.
*** <2015-12-07 Mon>
    + Results for find-decode-byte are (34 byte string):
    #+begin_src
WARNING: Final GC required 23.942862803765088 % of runtime
Evaluation count : 12 in 6 samples of 2 calls.
             Execution time mean : 69.644419 ms
    Execution time std-deviation : 1.923525 ms
   Execution time lower quantile : 67.551807 ms ( 2.5%)
   Execution time upper quantile : 71.815317 ms (97.5%)
                   Overhead used : 1.495886 ns
    #+end_src
    +Results for (score-byte-on-code dcb 120) (34 B string):
    #+begin_src
WARNING: Final GC required 37.60666036507942 % of runtime
Evaluation count : 2178 in 6 samples of 363 calls.
             Execution time mean : 273.585496 탎
    Execution time std-deviation : 979.771885 ns
   Execution time lower quantile : 272.300182 탎 ( 2.5%)
   Execution time upper quantile : 274.663612 탎 (97.5%)
                   Overhead used : 1.495886 ns
Evaluation count : 220560 in 60 samples of 3676 calls.
             Execution time mean : 275.487018 탎
    Execution time std-deviation : 2.555552 탎
   Execution time lower quantile : 271.487064 탎 ( 2.5%)
   Execution time upper quantile : 280.615661 탎 (97.5%)
                   Overhead used : 1.495886 ns

Found 2 outliers in 60 samples (3.3333 %)
        low-severe       2 (3.3333 %)
 Variance from outliers : 1.6389 % Variance is slightly inflated by outliers
    #+end_src
    +Results for modified score-byte-on-code. Removed relatives. Is
    increased because have added Chi-Square calculation.
    #+begin_src
Evaluation count : 229140 in 60 samples of 3819 calls.
             Execution time mean : 301.395905 탎
    Execution time std-deviation : 48.365102 탎
   Execution time lower quantile : 260.841685 탎 ( 2.5%)
   Execution time upper quantile : 368.774154 탎 (97.5%)
                   Overhead used : 1.495886 ns
    #+end_src
*** <2015-12-08 Tue>
    + Fixed the program and re-ran benchmark
    #+begin_src
Evaluation count : 1684860 in 60 samples of 28081 calls.
             Execution time mean : 41.641212 탎
    Execution time std-deviation : 6.201605 탎
   Execution time lower quantile : 34.396472 탎 ( 2.5%)
   Execution time upper quantile : 50.699054 탎 (97.5%)
                   Overhead used : 1.495886 ns
    #+end_src
    + Re-run (crit/bench (find-decode-byte dcb)) - I modified this to
      choose the minimum because of the new chi function
    #+begin_src
WARNING: Final GC required 33.70386639794799 % of runtime
Evaluation count : 90 in 6 samples of 15 calls.
             Execution time mean : 6.641849 ms
    Execution time std-deviation : 125.487250 탎
   Execution time lower quantile : 6.535929 ms ( 2.5%)
   Execution time upper quantile : 6.782285 ms (97.5%)
                   Overhead used : 1.495886 ns
    #+end_src
    + Need to be careful with state of code. Have broken quite a few
      things. But, on the plus side I think I've sped it up an order
      of magnitude - at least.
*** <2015-12-09 Wed>
    + Checked in, and now cleaning up the code. There was a lot of
      extra stuff in there. Also, my tests were closely coupled to the
      workings of my code - not the externals "API". Need to watch
      this in the future. - I've broken something on the
      find-decode-byte code.
    + Found issue. It was silly mistake. Had new line characters \r\n
      wrong way around. Easy way to remember is that n does nothing
      na( \r\na)
*** <2015-12-10 Thu>
    + Cleaning code a bit more. Have cleaned the letter frequency
      stuff. It is now two sequences. One bytes. Ones relatives. I
      then have function that will multiply the numbers and zip the
      two. This should be faster.
    + I've run the Challenge 6 manually. I found the decode bytes, but
      not the correct length (automatically, i found it by eye -
      around 38B I think). My problem is in the hamming distance
      average function. Not dealing with seq of seq properly. I'm sure
      I've made this more complex than need.
    + Wondering if I can use matrix operations to do the hamming
      distance work. It feels like it could fit...
*** <2015-12-11 Fri>
    + Did some minor work looking at challenge 6. This is still the focus.
*** <2015-12-12 Sat>
    + Break apart the Key finding sequence, and block sequence. They
      need work.
    + Have successfully decoded the challenge, but... I'm not scoring
      the correct prase as the best english. Not sure why. The invalid
      keys are giving 8080, while the valid one is a lot
      larger. Suspect the error is in the scoring function, but where...
    + Found the error in my scoring function. It was because I'd
      introduced a function to convrt chars to lower case. I just OR'd
      the byte with 0x20. However, this made 0x00 == 0x20 which
      wrongly scored some text. Need to go back an rework this
      function so that below 0x20 is not upgraded to display/normal
      characters.
    + Have finished Challenge 6. It is mess, and nasty, but
      done. Definately needs some refactoring.
**** Tried using GIT on emacs - magit-status                       :WORKFLOW:
     M-x magit-status to see git status, and in the status buffer:
     s to stage files
     c c to commit (type the message then C-c C-c to actually commit)
     b b to switch to another branch

     Other handy keys:

     P P to do a git push
     F F to do a git pull
     try to press TAB
*** <2015-12-13 Sun>
    + Grayson to hospital :(
    +
** AES in ECB Mode
*** <2015-12-14 Mon>
    + Have been struggling with OpenSSL or a bit. In the end I think
      the problem is due to character encoding of password. The file
      was encrypted using ASCII, but windows is converting my password
      text to unicode. This is a guess, but I kind of proove it with
      the following tests on command line:
      - I can encrypt and decrypt:
    #+begin_src
openssl enc -e -base64 -aes-128-ecb -in text.txt -k "PASS" -out test.enc
openssl enc -d -base64 -aes-128-ecb -in test.enc -k "PASS"
    #+end_src
      - I can decrypt the challenge 7.txt file as follows (note: the
        hex is the ascii code for "YELLOW SUBMARINE". Also K denotes
        pass phrase in Hex.
      #+begin_src
openssl enc -d -base64 -aes-128-ecb -in 7.txt -K 59454c4c4f57205355424d4152494eq45
      #+end_src
    + The output of 7.txt is the lyrics for Funky Music... just so I know.
    + First part of challenge is playing with and understanding Java
      Cryptography Architecture (JCA)
**** JCA
     + http://docs.oracle.com/javase/7/docs/technotes/guides/security/crypto/CryptoSpec.html
     + In java-clojure this was quite painless. A bit of back and
       forth between byte arrays, but quite easy.
     #+begin_src clojure
(import javax.crypto.Cipher javax.crypto.spec.SecretKeySpec)

(def k (into-array Byte/TYPE (map byte "YELLOW SUBMARINE"))) ;corce to byte
(def code-key (SecretKeySpec. k "AES")) ; generate code key (algorithm AES)
(def cipher (Cipher/getInstance "AES/ECB/NoPadding")) ; create AES cipher
(.init cipher Cipher/DECRYPT_MODE code-key) ; initalise cipher to decrypt
;; Decode from base64 the 7.txt and put in code-text
(def ct (into-array Byte/TYPE code-text)) ; coerce to to byte
(def plain-text (.doFinal c cd)) ; plain text as byte-array
(apply str  (map char plain-text)) ; coerce to string
     #+end_src
     + Next step is convert to clojure idiom... but first cleanup..
     + I've benchmarked my to-lower-digit vs java character
       convert. Mine is clear winer... (by 3 orders magnitude)
     #+name: my version of to-lower-digit
     #+begin_src clojure
(defn to-lower-digit
  [x]
  (let [y (short x)]
    (->
     y
     (bit-shift-right 1)
     (bit-and 0x20)
     (bit-or y))))
     #+end_src
     + Then the two tests, and their outputs.
     #+name: performance tests
     #+begin_src clojure
(crit/quick-bench (short (Character/toUpperCase (char 120))))
;WARNING: Final GC required 62.247873120205156 % of runtime
;Evaluation count : 72084 in 6 samples of 12014 calls.
;             Execution time mean : 10.439937 탎
;    Execution time std-deviation : 2.022481 탎
;   Execution time lower quantile : 8.252479 탎 ( 2.5%)
;   Execution time upper quantile : 12.874142 탎 (97.5%)
;                   Overhead used : 1.495886 ns

(crit/quick-bench (to-lower-digit 120))
;WARNING: Final GC required 72.32337594762932 % of runtime
;Evaluation count : 29719944 in 6 samples of 4953324 calls.
;             Execution time mean : 18.815455 ns
;    Execution time std-deviation : 0.185421 ns
;   Execution time lower quantile : 18.590476 ns ( 2.5%)
;   Execution time upper quantile : 19.036307 ns (97.5%)
;                   Overhead used : 1.495886 ns

;Found 1 outliers in 6 samples (16.6667 %)
;        low-severe       1 (16.6667 %)
; Variance from outliers : 13.8889 % Variance is moderately inflated by outliers
     #+end_src
     + compare (* x x) to (math/expt x 2)... 100 times faster using *
     #+begin_src clojure
clojure-crypto-challenge.core> (quick-bench (* 112 112))
;WARNING: Final GC required 72.27999511929572 % of runtime
;Evaluation count : 70409478 in 6 samples of 11734913 calls.
;             Execution time mean : 9.917522 ns
;    Execution time std-deviation : 0.293207 ns
;   Execution time lower quantile : 9.680164 ns ( 2.5%)
;   Execution time upper quantile : 10.380881 ns (97.5%)
;                   Overhead used : 1.495886 ns

;Found 1 outliers in 6 samples (16.6667 %)
;        low-severe       1 (16.6667 %)
; Variance from outliers : 13.8889 % Variance is moderately inflated by outliers

clojure-crypto-challenge.core> (quick-bench (math/expt 112 2))
;WARNING: Final GC required 59.721369753609075 % of runtime
;Evaluation count : 6303684 in 6 samples of 1050614 calls.
;             Execution time mean : 98.752183 ns
;    Execution time std-deviation : 9.690157 ns
;   Execution time lower quantile : 93.622400 ns ( 2.5%)
;   Execution time upper quantile : 115.228119 ns (97.5%)
;                   Overhead used : 1.495886 ns

;Found 1 outliers in 6 samples (16.6667 %)
;        low-severe       1 (16.6667 %)
; Variance from outliers : 30.3282 % Variance is moderately inflated by outliers
     #+end_src
     + I've sped up the line decode by limiting it. However, have to
       be careful. At the moment this is a static constant (say
       30). But if this is less than the Block of the cypher, then the
       scoring will not work. Can see effect of this by choosing
       length 25 and applying to Challenge 7.
*** <2015-12-15 Tue>
    + Reading about using arrays with map and reduce. Think it might
      be quicker for Score English task.
*** <2015-12-16 Wed>
    + Have created little array routine for frequency. Not benchmarked
      it yet..
    #+begin_src clojure
(def a (.getBytes "AAABXxlllyz")) ;Test string
(def cc (int-array 128 0)); frequency array

(dotimes [i (alength ^bytes a)]
(aset-int
  ^ints cc
  (aget ^bytes a i)
  (unchecked-inc (aget ^ints cc (aget ^bytes a i)))))

(println (apply str cc))
    #+end_src
