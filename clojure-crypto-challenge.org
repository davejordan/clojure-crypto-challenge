* Overview
  This is my attempt at "the matasano crypto challenges" at
  cryptopals.com

* Set 1
** Convert hex to base64
*** 2015-11-01
    Base64 uses 6 bits, vs normal hex (Base16) which uses 4. Need to
    convert bytes (8 bits, Base256) to 6 bit words. Best conversion is
    24 bits which is 3x8 and 4x6.

*** 2015-11-02
    Break byte array into hexlets.
    Next: need to refactor convert-hexlet!
    Saw an interesting way to manage the Base64code. Make an array of
    all characters

*** 2015-11-03
    Use partition function to split string into 3 byte groups. Then pad
    the tuples(?) with "0"s. I tried the following code, but removed
    it in preference for another version:
    #+BEGIN_SRC clojure
   (defn hex-3split
  [[b1 b2 b3]]
  (let [x (bit-or
           (bit-shift-left (int b1) 16)
           (bit-shift-left (int b2) 8)
           (bit-shift-left (int b3) 0))]
    [(unsigned-bit-shift-right x 18)
     (bit-and (unsigned-bit-shift-right x 12) 0x3f)
     (bit-and (unsigned-bit-shift-right x 6) 0x3f)
     (bit-and x 0x3f)]))
    #+END_SRC

*** 2015-11-04
    Think I've got base64-encode working. I'm now starting on base16
    decode.
    I've used a Java string conversion routine for hex conversion. I
    got set 1 test to pass!


** Fixed XOR
***   2015-11-04
      Started and finished. Just needed to add a formatting encode-base16 as
      well.

** Single-byte XOR cipher
***   2015-11-04
    Interesting! Need to break a cipher. Suggested to use character
    frequency, so need to look that up...
    Letter frequency table came from:
    http://www.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html
    My idea to score the result text is:
1. calculate score for each letter in text
2. calcualte relative score for each letter
3. calculate absolute difference in relative score for each letter (between
   known frequency and text)
4. sum difference
5. smallest difference wins

*** 2015-11-05
    Looking for a way to convert between string and keyword in
    clojure. Found "keyword", but not sure it's what I want.
    Another way to solve this might be to iterate over list and filter on
    each item in the iteration. This means at most 26ish repetitions (not
    sure about digits).
    Created function to count occurences of characters in a string and
    return character and count in map.

*** 2015-11-06
    Got word-score working. It can be a generic function that compares
    two maps, comparing the second value of each map.
    I hacked a bit of a pay around to try and get the algorithm. CAN'T
    LEAVE IT LIKE THIS!
*** 2015-11-07
    I've seen the code decrypted, but my frequency algorithm is not
    quite working correctly. It may be that I'm not creating a
    distribution of the code letters, and am just comparing absoute
    counts. Also, not sure how to account for punctuation in frequency
    distribution.
    Cleaning up some of the earlier code. Especially the base64
    decode.
*** <2015-11-08>
    + Detour - play around with concept of tranducers. Started
      watching R.Hickey talk on this. Recommends "Lectures on
      Constructive Functional Programming" by R.S
      Bird. http://www.cs.ox.ac.uk/files/3390/PRG69.pdf and A tutorial
      on the universality and expressiveness of fold by GRAHAM HUTTON
      -- http://www.cs.nott.ac.uk/~gmh/fold.pdf
    + I've just commited change with commented old functions. I will
      now remove these, and just leave transducers.
    + I was really sick of the encode-byte routine. So I ran it, and
      copied the output directly in. Much cleaner. For reference the
      original was:
      #+BEGIN_SRC clojure
        (defn encode-byte-base64
          [b]
          (let [crs [[\A \Z] [\a \z] [\0 \9]]
                rfn (fn [[a b]] (range (byte a) (inc (byte b))))
                rconj (fn [x y m] (conj (vec  m) x y))]
            (nth (->>
                  crs
                  (map rfn)
                  (rconj \+ \/)
                  flatten
                  (map char)) b)))
      #+END_SRC
      + Found function "frequencies" which counts distinct occurences
        of items in sequence. Replaced Count-occurences. Previous code
        was:
      #+BEGIN_SRC clojure
        (defn count-occurences
          [m l]
          (let [h (first l)
                f (fn [a] (= h a))]
            (cond
              (some? h) (count-occurences
                         (assoc m h (count (filter f l)))
                         (remove f l))
              :else m)))

        (deftest test-count-occurences
          (testing "test count-occurence"
            (are [x y] (= x (count-occurences {} y))
              {\a 1} "a"
              {\a 5} "aaaaa"
              {} ""
              {\a 3 \b 3} "ababab")))
      #+END_SRC
*** <2015-11-09>
    + I need to create a function that will diff two maps. Should
      return either a map, or a sequence of differences.
      1. Use the input map as base
      2. for each key, output a new key that is value - value of same
         key in reference map
      3. If reference map does not have the key, then assume the
         referene map has 0 as the value.
    + I've found the merge-with function. this is perfect for my
      needs. It takes two maps, and merges them while applying a
      function.
*** <2015-11-10>
    + Finding it hard to create a test for the XOR word
      function. There's definately something wrong with my scoring
      function. I'm just not sure what. Why is everything scoring
      9.9172..?
*** <2015-11-11>
    + I think i've got character conversion issues. Not sure how my
      tests haven't picked this up. I'll have to review, and find
      why. Still not getting a good word score!
    + Finally got it working. Was a mix of dealing with special
      characters correctly and a good comparison function. Trick was
      to remove whitespace, but leave non-letters.This penalises
      non-letters but ignore whitespace. Also, comparison function is
      divide. This works better than - gives a better differentiator.
    + Code needs a clean-up. Maybe some more tests too?? Definately a
      clean up.
    + Anyway, challenge complete!
** Detect single-character XOR
*** <2015-11-12>
    + Probably the main steps for this challenge are:
      - Slurp file
      - For each line in file find the decode-byte and word score
      - Return the line, byte and score for every line
      - Take the line with the maximum score
    + Did a bit of clean-up and checked in.
*** <2015-11-19>
    + Can make this Lazy I think. Run the decoder in the file read
      loop.
    + I've hacked together a solution, but it is pretty slow. I'm sure
      i can come up with something much better.
    + The challenge test runs slow (1000ms?), so I've disabled the
      test. Again I think this is better servered with a refactor.
    + I'm still not convinced about my code breakdown. I want more
      useful, small functions I can tie together.
** Implement repeating-key XOR
*** <2015-11-19>
    + I have a small error where the Hex string representation is not
      including leading 0s on conversions
    + I changed the Hex encode function. Was simply change in string
      from "%h" to "%02x".
    + The text was correct, but a hidden \n threw me off.
    + Challenge complete
** Break repeating-key XOR
*** <2015-11-20>
    + First task is to get Hamming distance between two words. I think
      this is straight forward using Integer.bitCount from Java. Yes,
      it was pretty simple.
    + To solve the main problem I just need to compose existing items:
      - Slurp file
      - Decode from base64
      - Find the keysize
        - Get the number between 2 and 40 with the minimum hamming
      - Break the code into blocks
      - Solve each block as single - keep the decode byte
      - Combine the decode bytes
    + I need to create a decode-base64 function. I've only got a
      decode function at the moment.
    + Actually, I found the Java 8 function
      Base64.Decoder.decode. Have created a wrapper that converts
      input to string, and output to vector. Nice and simple.
    + I've put together a skeleton, but it's not right. I've run it,
      and get jibberish results. Not sure what's going wrong. Assume
      the key i'm discovering is wrong, as I'd expect challenge to use
      an actual word as a key. The current key I'm getting is :[95 83
      94 84 94]
*** <2015-11-27>
    + It's been a while. I've been working on a presentation for RSS,
      and in the mean time I have learnt a lot about R statistics
      language. I think I will give it a go plotting some of the
      output of my algoriths.
    + I plotted output. The outlier is not very clear. I'm wondering
      if my frequency difference algorithm needs tweaking. I'll have a
      look at this.
*** <2015-11-28>
    + Started twaking a copy of my frequency difference table. Not
      ready yet, so I commented it back out.
*** <2015-11-29>
    + Have been playing a lot with R on this. Noticed some interesting
      things. The SD and variance on letter frequencies is the same
      for every encoded string. This makes sense because we're
      modifing all letters exactly the same way with the XOR
      byte.
      This leads to the simple conclusion that we can't score phrases
      as english without taking into consideration the frequencies of
      actual letters (i.e. can't just score based on spread, mean, etc).
    + Have also found, which I'd ignored, the frequencies for spaces,
      white-space, and other punctuation. The "Space" is
      1.918182% (this treats carriage return as space). In fact, all
      non-letter characters (including space) represent 6.57%.
    + Next nice little exercise in R would be to load in the letter
      frequencies and actually graph a comparison.
*** <2015-11-30>
    + Have got a simple comparison of known letter frequency and
      decoded message letter frequency. The method I was using does
      not give a great answer. There is no "stand-out" winner, in
      terms of phrase score.
*** <2015-12-01>
    + Looking at score distributions, I think I really need a way to
      take non-valid characters into account. Some sort of score
      penalty for each non-printable character? Or maybe subtract
      non-printable characters from the total. Or maybe subtract all
      whitespace, except " ", from total?
*** <2015-12-02>
    + I've tried a Chi-Test on this, and also removed the comparison
      of relative (by multiplying by string length C=34). I'm still
      getting two out-liers. This definately needs a extra test on the
      types of characters included.
    + Another type of plot that will be interesting is character
      spread coloured by score. This will mean the columns are
      coloured, but the rows will show which characters are being
      used. I could also colour the values by character range..
*** <2015-12-03>
    + I've modified my Chi-Test. It now uses two values for NA in the
      expected value. It uses 0 in numerator, and 1 in
      denominator. Currently, this is two conditionals, but I'm sure
      can make this better. Anyway, result is now have a better
      outlier (minimum in this case) to identify.
*** <2015-12-05 Sat>
    + I did quite a bit last night, but didn't journal it. Not a good
      habbit to get into!
    + Last night I cleaned up the R code. Also, I cleared the
      buffer. This was actually a long task, as the buffer was huge
      (300k?). I had to force emacs to close, as it was unresponsive.
    + I've removed a lot of the junk from core.r. Down to 91 lines
      including plot.
    + The plot is now showing progression of the score over each
      iteration. This is interesting - the winning score does not
      shine until quite late in the scoring. I wonder if there is a
      better way? Or maybe a way to rule out sequences early in the
      process? Another idea is maybe create a bounding function that
      excludes falses early.
*** <2015-12-06 Sun>
    + I've installed Criterium. This is a clojure benchmarking
      tool. The idea being I can collect and benchmark the various
      algorithms for finding english
      phrases. https://github.com/hugoduncan/criterium/
    + Powershell command number lines in file:
      #+begin_src
      Get-Content test/clojure_crypto_challenge/4.txt | Measure-Object -Line
      #+end_src
    + Just found problem in core.clj. Lucky for source control. Was
      able to compare against previous commit. For future reference
      the command was "
      #+begin_src
      git diff HEAD^1 -- .\src\clojure_crypto_challenge\core.clj"
      #+end_src
    + I modified the R program, and think I have a nicer
      function. Instead of using the actual expected value of the
      letters - which for small strings always evaluates less than 1 -
      I use a rounded value. There are differences between ceiling,
      floor, and round. Round works the best - but must be carefull to
      treat "0" as "NA" (so the Chi score = x^2, not (x-y)^2/y -which
      would go infinity).
      + Also, the bounding function seems to be a straight line. Maybe
        at 2.5? Would need to tweak this, as it will be skewed by
        selected sample.
    +
